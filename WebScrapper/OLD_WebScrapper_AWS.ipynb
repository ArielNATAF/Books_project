{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "executionInfo": {
     "elapsed": 977,
     "status": "ok",
     "timestamp": 1616238784731,
     "user": {
      "displayName": "Carine Therond",
      "photoUrl": "",
      "userId": "01656088634002615900"
     },
     "user_tz": -60
    },
    "id": "oEYnvg77_aRY"
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named google.colab",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m\u001b[0m",
      "\u001b[1;31mImportError\u001b[0mTraceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-46a720cc60e6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m: No module named google.colab"
     ]
    }
   ],
   "source": [
    "from google.colab import drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": false,
    "executionInfo": {
     "elapsed": 18245,
     "status": "ok",
     "timestamp": 1616238803755,
     "user": {
      "displayName": "Carine Therond",
      "photoUrl": "",
      "userId": "01656088634002615900"
     },
     "user_tz": -60
    },
    "id": "6_YCxDAk_g5X",
    "outputId": "e9486700-f8de-40ab-ddab-886197c2be17"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": false,
    "executionInfo": {
     "elapsed": 940,
     "status": "ok",
     "timestamp": 1616238807749,
     "user": {
      "displayName": "Carine Therond",
      "photoUrl": "",
      "userId": "01656088634002615900"
     },
     "user_tz": -60
    },
    "id": "lKY85y1fAa59"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import re\n",
    "import requests\n",
    "#from urllib.request import urlopen\n",
    "import urllib3\n",
    "import bs4\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "executionInfo": {
     "elapsed": 1066,
     "status": "ok",
     "timestamp": 1616238810123,
     "user": {
      "displayName": "Carine Therond",
      "photoUrl": "",
      "userId": "01656088634002615900"
     },
     "user_tz": -60
    },
    "id": "0Mhb2m42_kzs"
   },
   "outputs": [],
   "source": [
    "def ParserCsvInputBookCrossing(currentLine):\n",
    "    L = currentLine.split('\"')\n",
    "    \n",
    "    return [L[i] for i in range(16) if i%2 == 1]\n",
    "\n",
    "    \n",
    "#    L = currentLine.split(';')\n",
    "#    if len(L) == 8:\n",
    "#        return L\n",
    "#        \n",
    "#    else:\n",
    "#        g = re.search('([0-9X]+);(.+);([0-9]{4,4})(.+)(;http.+.jpg)+(;http.+.jpg)+(;http.+.jpg)+', currentLine)\n",
    "#\n",
    "#        g_2 = g[2].split('\"')\n",
    "#        if len(g_2) == 1:\n",
    "#            g_2_12 = g[2].split(';')\n",
    "#        else:\n",
    "#            g_2_12 = [g_2[i] for i in range(len(g_2)) if not (g_2[i] == '' or g_2[i] == ';')]\n",
    "#     \n",
    "#        return [x.strip(';') for x in [g[1], g_2_12[0], g_2_12[1], g[3], g[4], g[5], g[6], g[7]]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "collapsed": false,
    "executionInfo": {
     "elapsed": 947,
     "status": "ok",
     "timestamp": 1616238813076,
     "user": {
      "displayName": "Carine Therond",
      "photoUrl": "",
      "userId": "01656088634002615900"
     },
     "user_tz": -60
    },
    "id": "rHkZYbYMAi2U"
   },
   "outputs": [],
   "source": [
    "def GoogleSearchFunction(isbn = None, title = None, author = None):\n",
    "    ''' Find information on internet about the book entered in function's arguments \n",
    "        Either isbn = ..., Or title = ..., author = ... '''\n",
    "    \n",
    "    #3 kinds of books identifiers\n",
    "    ISBNdict = {\"ISBN_10\": 0, \"ISBN_13\": 0, \"OtherID\": 0}\n",
    "    \n",
    "    #all is reset\n",
    "    ISBNdict[\"ISBN_10\"] = 0.0\n",
    "    ISBNdict[\"ISBN_13\"] = 0.0   \n",
    "    ISBNdict[\"OtherID\"] = 0.0\n",
    "    autGoogle = []\n",
    "    titleSearched = ''\n",
    "    autSearched = ''\n",
    "    pubSearched = ''\n",
    "    pubData = ''\n",
    "    catSearched = ''\n",
    "    descSearched = ''\n",
    "    langSearched = ''\n",
    "    imageSearched = ''\n",
    "    pageSearched = 0\n",
    "\n",
    "    ISBNfound = False\n",
    "    AUTHORfound = False\n",
    "    repGoogle = ''\n",
    "    \n",
    "    try:\n",
    "        #book is searched on internet thanks to its ISBN_10 identifier\n",
    "        if (isbn is not None):\n",
    "            \n",
    "            ISBNdict[\"ISBN_10\"] = isbn\n",
    "\n",
    "            #Request format for a search on Google Book web site\n",
    "            #=> We have to format the ISBN so that it is written on 10 digits => {0:0>10s}\n",
    "            requeteIsbnGoogle = \"https://www.googleapis.com/books/v1/volumes?q=isbn:{0:0>10s}\".format(ISBNdict[\"ISBN_10\"])\n",
    "    \n",
    "            try:\n",
    "                #In case where a book reference has been found\n",
    "                repGoogle = pd.read_json(requeteIsbnGoogle)\n",
    "                ISBNfound = True\n",
    "\n",
    "            except:\n",
    "                #In case where a book reference has NOT been found                \n",
    "                repGoogle = pd.read_json(requeteIsbnGoogle, typ = 'series')\n",
    "                ISBNfound = False\n",
    "        \n",
    "        #book is searched on internet thanks to its title and 1st author\n",
    "        else:\n",
    "            if ((title != None) and (author != None)):\n",
    "\n",
    "                titleSearched = title.encode('ascii', 'ignore').decode('ascii')\n",
    "                autSearched = author.encode('ascii', 'ignore').decode('ascii')\n",
    "                #We keep only the author familly name\n",
    "                autSearchedInternal = autSearched.lower().split()[-1]\n",
    "\n",
    "                #Request format for a search on Google Book web site\n",
    "                #=> We must take care of ' (replace('\\'', '')) and space (replace(' ','%20'))\n",
    "                requeteTitleGoogle = \"https://www.googleapis.com/books/v1/volumes?q=title:%s\" % (titleSearched.replace('\\'', '').replace(' ','%20'))\n",
    "    #           print(requeteTitleGoogle)\n",
    "                repGoogle = pd.read_json(requeteTitleGoogle) \n",
    "\n",
    "                #The Title has been found on Google Books\n",
    "                #We only analyse the first book's reference retrieved => repTitleGoogle['items'][0]\n",
    "                if (repGoogle['totalItems'][0] != 1):\n",
    "\n",
    "                    if ('authors' in repGoogle['items'][0]['volumeInfo']):\n",
    "\n",
    "                        #All authors proposed by Google\n",
    "                        autGoogle = repGoogle['items'][0]['volumeInfo']['authors']\n",
    "\n",
    "                        #Among all authors proposed by Google, we search correspondance with the first provided author\n",
    "                        j = 0\n",
    "                        while ((j < len(autGoogle)) and (AUTHORfound == False)):\n",
    "\n",
    "                            if (autSearchedInternal in autGoogle[j].lower()):\n",
    "                                AUTHORfound = True  \n",
    "                                ISBNfound = True\n",
    "\n",
    "                            j += 1\n",
    "\n",
    "        #ISBN or title/author has been recognized on Google Books\n",
    "        if (ISBNfound == True):\n",
    "            titleSearched = repGoogle['items'][0]['volumeInfo']['title']\n",
    "\n",
    "            if 'subtitle' in repGoogle['items'][0]['volumeInfo']:\n",
    "                titleSearched = titleSearched + ' : ' + repGoogle['items'][0]['volumeInfo']['subtitle']\n",
    "\n",
    "            #'industryIdentifiers' stands for ISBN_10, ISBN_13 or OtherID\n",
    "            if ('industryIdentifiers' in repGoogle['items'][0]['volumeInfo']):\n",
    "\n",
    "                #Among all identifiers type proposed by Google, we search \"ISBN_10\", \"ISBN_13\" or \"OtherID\"\n",
    "                for k in range(len(repGoogle['items'][0]['volumeInfo']['industryIdentifiers'])):\n",
    "\n",
    "                    if repGoogle['items'][0]['volumeInfo']['industryIdentifiers'][k]['type'] == \"ISBN_10\":\n",
    "                        ISBNdict[\"ISBN_10\"] = repGoogle['items'][0]['volumeInfo']['industryIdentifiers'][k]['identifier']\n",
    "                        ISBNfound = True\n",
    "\n",
    "                    elif repGoogle['items'][0]['volumeInfo']['industryIdentifiers'][k]['type'] == \"ISBN_13\":\n",
    "                        ISBNdict[\"ISBN_13\"] = repGoogle['items'][0]['volumeInfo']['industryIdentifiers'][k]['identifier']\n",
    "                        ISBNfound = True\n",
    "\n",
    "                    else:\n",
    "                        ISBNdict[\"OtherID\"] = repGoogle['items'][0]['volumeInfo']['industryIdentifiers'][k]['identifier']\n",
    "                        ISBNfound = True\n",
    "\n",
    "\n",
    "            if ('authors' in repGoogle['items'][0]['volumeInfo']):\n",
    "                autSearched = repGoogle['items'][0]['volumeInfo']['authors'][0]\n",
    "\n",
    "            if ('publisher' in repGoogle['items'][0]['volumeInfo']):\n",
    "                pubSearched = repGoogle['items'][0]['volumeInfo']['publisher']\n",
    "\n",
    "            if ('categories' in repGoogle['items'][0]['volumeInfo']):\n",
    "                catSearched = repGoogle['items'][0]['volumeInfo']['categories'][0]\n",
    "\n",
    "            if ('searchInfo' in repGoogle['items'][0]):\n",
    "                descSearched = repGoogle['items'][0]['searchInfo']['textSnippet']                    \n",
    "\n",
    "            if ('language' in repGoogle['items'][0]['volumeInfo']):\n",
    "                langSearched = repGoogle['items'][0]['volumeInfo']['language']\n",
    "\n",
    "            if ('imageLinks' in repGoogle['items'][0]['volumeInfo']):\n",
    "                imageSearched = repGoogle['items'][0]['volumeInfo']['imageLinks']['smallThumbnail']\n",
    "\n",
    "            if ('pageCount' in repGoogle['items'][0]['volumeInfo']):\n",
    "                pageSearched = repGoogle['items'][0]['volumeInfo']['pageCount']\n",
    "\n",
    "            if ('publishedDate' in repGoogle['items'][0]['volumeInfo']):\n",
    "                pubData = repGoogle['items'][0]['volumeInfo']['publishedDate']\n",
    "                pubData = re.search(\"[0-9]{4,4}\", pubData).group(0)\n",
    "\n",
    "        return (ISBNfound, [ISBNdict[\"ISBN_10\"], ISBNdict[\"ISBN_13\"], ISBNdict[\"OtherID\"], titleSearched, autSearched, \\\n",
    "                               pubData, pubSearched, catSearched, descSearched, langSearched, imageSearched, pageSearched, '', '', []])\n",
    "\n",
    "    except:\n",
    "        return (False, [ISBNdict[\"ISBN_10\"], ISBNdict[\"ISBN_13\"], ISBNdict[\"OtherID\"], titleSearched, autSearched, \\\n",
    "                           pubData, pubSearched, catSearched, descSearched, langSearched, imageSearched, pageSearched, '', '', []])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "collapsed": false,
    "executionInfo": {
     "elapsed": 710,
     "status": "ok",
     "timestamp": 1616238817862,
     "user": {
      "displayName": "Carine Therond",
      "photoUrl": "",
      "userId": "01656088634002615900"
     },
     "user_tz": -60
    },
    "id": "cgzdUBJjAmm8"
   },
   "outputs": [],
   "source": [
    "def GoodReadsSearchFunction(isbn = None, title = None, author = None):\n",
    "    ''' Find information on internet about the book entered in function's arguments \n",
    "        Either isbn = ..., Or title = ..., author = ... '''\n",
    "    \n",
    "    #3 kinds of books identifiers\n",
    "    ISBNdict = {\"ISBN_10\": 0, \"ISBN_13\": 0, \"OtherID\": 0}\n",
    "    \n",
    "    #all is reset\n",
    "    ISBNdict[\"ISBN_10\"] = 0.0\n",
    "    ISBNdict[\"ISBN_13\"] = 0.0   \n",
    "    ISBNdict[\"OtherID\"] = 0.0\n",
    "    autGoodReads = ''\n",
    "    titleSearched = ''\n",
    "    autSearched = ''\n",
    "    pubSearched = ''\n",
    "    pubData = ''\n",
    "    catSearched = ''\n",
    "    descSearched = ''\n",
    "    awardsSearched = ''\n",
    "    languageSearched = ''\n",
    "    pagesSearched = 0\n",
    "    authorGenreSearched = ''\n",
    "    goodreadsLinksSeriesList = []\n",
    "\n",
    "    AUTHORfound = False\n",
    "    ISBNfound = False\n",
    "    \n",
    "    try:\n",
    "        if(isbn is not None):\n",
    "            #https://www.goodreads.com/search?q=195153448&qid=\n",
    "            ISBNdict[\"ISBN_10\"] = isbn\n",
    "    \n",
    "            urlIdBookGoodReads_start = \"https://www.goodreads.com/search?q={0:0>10s}\".format(ISBNdict[\"ISBN_10\"])\n",
    "            urlIdBookGoodReads_end = '&qid='\n",
    "            bookUrlForSearch = urlIdBookGoodReads_start + urlIdBookGoodReads_end\n",
    "            \n",
    "            #source = urlopen(bookUrlForSearch)\n",
    "            #source = requests.get(bookUrlForSearch).content\n",
    "            http = urllib3.PoolManager()\n",
    "            source = http.request('GET', bookUrlForSearch).data\n",
    "            soup = bs4.BeautifulSoup(source, 'html.parser')\n",
    "            ISBNfound = (soup.find(\"h1\", id=\"bookTitle\") != None)\n",
    "\n",
    "        else:    \n",
    "            if ((title != None) and (author != None)):\n",
    "                urlIdBookGoodReads_start = 'https://www.goodreads.com/search?utf8=%E2%9C%93&q='\n",
    "                urlIdBookGoodReads_end = '&search_type=books'\n",
    "                \n",
    "                titleSearched = title.encode('ascii', 'ignore').decode('ascii')\n",
    "                autSearched = author.encode('ascii', 'ignore').decode('ascii')\n",
    "                #We keep only the author familly name\n",
    "                autSearchedInternal = autSearched.lower().split()[-1]\n",
    "                \n",
    "                #First we have to find the book page: we need GoodReads identifier of the book \n",
    "                bookUrlForSearch = urlIdBookGoodReads_start + titleSearched.replace(' ', '+') + '+' + autSearchedInternal.replace(' ', '+') + urlIdBookGoodReads_end\n",
    "#                print(bookUrlForSearch)\n",
    "                \n",
    "                #sourceIdBookGoodReads = urlopen(bookUrlForSearch)\n",
    "                sourceIdBookGoodReads = requests.get(bookUrlForSearch).content\n",
    "                soupIdBookGoodReads = bs4.BeautifulSoup(sourceIdBookGoodReads, 'html.parser')\n",
    "                \n",
    "                #Search of the reference compliant with title and author provided, among all proposed books references\n",
    "                booksReference = soupIdBookGoodReads.find_all(\"a\", class_=\"bookTitle\")\n",
    "                j = 0\n",
    "                while ((j < len(booksReference)) and (AUTHORfound == False)):\n",
    "                    autGoodReads = booksReference[j].find_next(\"a\", class_=\"authorName\").text\n",
    "                    \n",
    "                    if (autSearchedInternal in autGoodReads.lower()):\n",
    "                        AUTHORfound = True\n",
    "                    j += 1    \n",
    "                \n",
    "                #Now we go to the selected book's page\n",
    "                if (AUTHORfound == True):\n",
    "                    #source = urlopen('https://www.goodreads.com' + booksReference[j-1][\"href\"]) \n",
    "                    source = requests.get('https://www.goodreads.com' + booksReference[j-1][\"href\"]).content\n",
    "                    soup = bs4.BeautifulSoup(source, 'html.parser')\n",
    "                    ISBNfound = True\n",
    "\n",
    "        #Now we are on the specific book's web page\n",
    "        #=> Common analysis begins\n",
    "        if ISBNfound:\n",
    "            titleSearched = soup.find(\"h1\", id=\"bookTitle\").text\n",
    "            titleSearched = titleSearched.replace('\\n', '').strip()\n",
    "            \n",
    "            #Search of ISBN (10 and / or 13), language and awards\n",
    "            searchForISBN_Lang_Awards_Series = soup.find(\"div\", id=\"bookDataBox\").find_all(\"div\", class_=\"clearFloats\")\n",
    "\n",
    "            for i in range(len(searchForISBN_Lang_Awards_Series)):\n",
    "                \n",
    "                if searchForISBN_Lang_Awards_Series[i].find(\"div\", \"infoBoxRowTitle\").text == 'ISBN':\n",
    "                    isbnFull = searchForISBN_Lang_Awards_Series[i].find(\"div\", \"infoBoxRowItem\").text.strip()\n",
    "\n",
    "                    ISBNdict[\"ISBN_10\"] = isbnFull.split(\"(ISBN13:\")[0].strip()\n",
    "                    \n",
    "                if searchForISBN_Lang_Awards_Series[i].find(\"div\", \"infoBoxRowTitle\").text == 'Edition Language':\n",
    "                    languageSearched = searchForISBN_Lang_Awards_Series[i].find(\"div\", \"infoBoxRowItem\").text\n",
    "                    #In order to be compliant with GoogleBooks names\n",
    "                    if (languageSearched == 'English'):\n",
    "                        languageSearched = 'en'\n",
    "            \n",
    "                if searchForISBN_Lang_Awards_Series[i].find(\"div\", \"infoBoxRowTitle\").text == 'Literary Awards':\n",
    "                    awardsSearched = searchForISBN_Lang_Awards_Series[i].find(\"div\", \"infoBoxRowItem\").text\n",
    "                    awardsSearched = awardsSearched.strip()\n",
    "            \n",
    "            #Search of published date and publisher       \n",
    "            searchForPublished = soup.find(\"div\", id=\"details\").find_all(\"div\", class_=\"row\")\n",
    "\n",
    "            for i in range(len(searchForPublished)):\n",
    "                if 'Published' in searchForPublished[i].text:\n",
    "                    #Current published date and additional information on first publication date is also provided\n",
    "                    if (searchForPublished[i].find(\"nobr\", class_=\"greyText\")):\n",
    "                        infoFirstPublished = searchForPublished[i].find(\"nobr\", class_=\"greyText\")\n",
    "                        tempPublished = searchForPublished[i].text.split(infoFirstPublished.text)\n",
    "                        tempPublished = tempPublished[0].strip().replace('\\n', '').replace('Published', '').split('by')\n",
    "                        pubData = tempPublished[0].strip()\n",
    "                        pubData = re.search(\"[0-9]{4,4}\", pubData).group(0)\n",
    "                        pubSearched = tempPublished[1].strip()\n",
    "                    #Only current published date is provided\n",
    "                    else:\n",
    "                        tempPublished = searchForPublished[i].text.strip().replace('\\n', '').replace('Published', '').split('by')\n",
    "                        pubData = tempPublished[0].strip()\n",
    "                        pubData = re.search(\"[0-9]{4,4}\", pubData).group(0)\n",
    "                        pubSearched = tempPublished[1].strip()\n",
    "\n",
    "            #Search of the pages number\n",
    "            searchForPages = soup.find(\"div\", id=\"details\").find_all(\"div\", class_=\"row\")\n",
    "            for i in range(len(searchForPages)):\n",
    "                if 'pages' in searchForPages[i].text:\n",
    "                    pagesSearched = searchForPublished[i].text\n",
    "                    pagesSearched = pagesSearched.split(',')[-1].split('pages')[0].strip()\n",
    "        \n",
    "            #Search for the book's genre\n",
    "            if (soup.find(\"a\", class_=\"actionLinkLite bookPageGenreLink\")):\n",
    "                catSearched = soup.find_all(\"a\", class_=\"actionLinkLite bookPageGenreLink\")[0].text\n",
    "            \n",
    "            #Search for knowing if book is part of a serie\n",
    "            #=> If yes, we keep the URL of the other mentionned books of the serie\n",
    "            if soup.find(\"div\", class_=\"seriesList\"):\n",
    "                if (soup.find(\"div\", class_=\"seriesList\").find_next(\"h2\").text == 'Other books in the series'):\n",
    "                    #We go to this specific book serie web page\n",
    "                    linkTowardSeries = soup.find(\"div\", class_=\"seriesList\").find_next(\"h2\").find_next(\"a\")[\"href\"]\n",
    "                    r = requests.get('https://www.goodreads.com' + linkTowardSeries)\n",
    "                    \n",
    "                    #We keep all the other books reference of this serie\n",
    "                    seriesList = re.findall('href=\"/book/show/[0-9]+', str(r.content))\n",
    "                    goodreadsLinksSeriesList = [seriesList[i].replace('href=\"', 'https://www.goodreads.com') for i in range(len(seriesList)) if i%2 == 0]\n",
    "            \n",
    "            #search of the author genre: we need to finf the URL of author page first\n",
    "            autSearched = soup.find(\"a\", class_=\"authorName\").text\n",
    "            #sourceAuthor = urlopen(soup.find(\"a\", class_=\"authorName\")[\"href\"])\n",
    "            sourceAuthor = requests.get(soup.find(\"a\", class_=\"authorName\")[\"href\"]).content\n",
    "            soupAuthor = bs4.BeautifulSoup(sourceAuthor, 'html.parser')\n",
    "            \n",
    "            #On the author web site page, we search for the author genre\n",
    "            searchForAuthorGenre = soupAuthor.find_all(\"div\", class_=\"dataTitle\")\n",
    "            for i in range(len(searchForAuthorGenre)):\n",
    "                if (searchForAuthorGenre[i].text == \"Genre\"):\n",
    "                    authorGenreSearched = searchForAuthorGenre[i].find_next(\"div\", class_=\"dataItem\").text.split(',')[0]\n",
    "                    authorGenreSearched = authorGenreSearched.replace('\\n', '').strip()\n",
    "    \n",
    "        return (ISBNfound, [ISBNdict[\"ISBN_10\"], ISBNdict[\"ISBN_13\"], ISBNdict[\"OtherID\"], titleSearched, autSearched, \\\n",
    "                pubData, pubSearched, catSearched, descSearched, languageSearched, '', pagesSearched, awardsSearched, \\\n",
    "                authorGenreSearched, goodreadsLinksSeriesList])\n",
    "    \n",
    "    except:\n",
    "        return (False, [ISBNdict[\"ISBN_10\"], ISBNdict[\"ISBN_13\"], ISBNdict[\"OtherID\"], titleSearched, autSearched, \\\n",
    "                pubData, pubSearched, catSearched, descSearched, languageSearched, '', pagesSearched, awardsSearched, \\\n",
    "                authorGenreSearched, goodreadsLinksSeriesList])    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "collapsed": false,
    "executionInfo": {
     "elapsed": 651,
     "status": "ok",
     "timestamp": 1616238984614,
     "user": {
      "displayName": "Carine Therond",
      "photoUrl": "",
      "userId": "01656088634002615900"
     },
     "user_tz": -60
    },
    "id": "ZNjC7xmzArX9"
   },
   "outputs": [],
   "source": [
    "def GoodReadsPartialSearchFunction(isbn =  None):\n",
    "    \n",
    "    awardsSearched = ''\n",
    "    authorGenreSearched = ''\n",
    "    goodreadsLinksSeriesList = []\n",
    "    \n",
    "    ISBNfound = False\n",
    "    \n",
    "    try:\n",
    "        urlIdBookGoodReads_start = \"https://www.goodreads.com/search?q={0:0>10s}\".format(isbn)\n",
    "        urlIdBookGoodReads_end = '&qid='\n",
    "        bookUrlForSearch = urlIdBookGoodReads_start + urlIdBookGoodReads_end\n",
    "        \n",
    "        #source = urlopen(bookUrlForSearch)\n",
    "        source = requests.get(bookUrlForSearch).content\n",
    "        soup = bs4.BeautifulSoup(source, 'html.parser')\n",
    "        ISBNfound = (soup.find(\"h1\", id=\"bookTitle\") != None)\n",
    "\n",
    "\n",
    "        #Now we are on the specific book's web page\n",
    "        if ISBNfound:\n",
    "            searchForISBN_Lang_Awards_Series = soup.find(\"div\", id=\"bookDataBox\").find_all(\"div\", class_=\"clearFloats\")\n",
    "            \n",
    "            for i in range(len(searchForISBN_Lang_Awards_Series)):\n",
    "                if searchForISBN_Lang_Awards_Series[i].find(\"div\", \"infoBoxRowTitle\").text == 'Literary Awards':\n",
    "                    awardsSearched = searchForISBN_Lang_Awards_Series[i].find(\"div\", \"infoBoxRowItem\").text\n",
    "\n",
    "                if searchForISBN_Lang_Awards_Series[i].find(\"div\", \"infoBoxRowTitle\").text == 'Edition Language':\n",
    "                    languageSearched = searchForISBN_Lang_Awards_Series[i].find(\"div\", \"infoBoxRowItem\").text\n",
    "                    #In order to be compliant with GoogleBooks names\n",
    "                    if (languageSearched == 'English'):\n",
    "                        languageSearched = 'en'\n",
    "\n",
    "            #Search for knowing if book is part of a serie\n",
    "            #=> If yes, we keep the URL of the other mentionned books of the serie\n",
    "            if soup.find(\"div\", class_=\"seriesList\"):\n",
    "                if (soup.find(\"div\", class_=\"seriesList\").find_next(\"h2\").text == 'Other books in the series'):\n",
    "                    #We go to this specific book serie web page\n",
    "                    linkTowardSeries = soup.find(\"div\", class_=\"seriesList\").find_next(\"h2\").find_next(\"a\")[\"href\"]\n",
    "                    r = requests.get('https://www.goodreads.com' + linkTowardSeries)\n",
    "                    \n",
    "                    #We keep all the other books reference of this serie\n",
    "                    seriesList = re.findall('href=\"/book/show/[0-9]+', str(r.content))\n",
    "                    goodreadsLinksSeriesList = [seriesList[i].replace('href=\"', 'https://www.goodreads.com') for i in range(len(seriesList)) if i%2 == 0]\n",
    "                    \n",
    "            #search of the author genre: we need to finf the URL of author page first\n",
    "            #sourceAuthor = urlopen(soup.find(\"a\", class_=\"authorName\")[\"href\"])\n",
    "            sourceAuthor = requests.get(soup.find(\"a\", class_=\"authorName\")[\"href\"]).content\n",
    "            soupAuthor = bs4.BeautifulSoup(sourceAuthor, 'html.parser')\n",
    "            \n",
    "            #On the author web site page, we search for the author genre\n",
    "            searchForAuthorGenre = soupAuthor.find_all(\"div\", class_=\"dataTitle\")\n",
    "            for i in range(len(searchForAuthorGenre)):\n",
    "                if (searchForAuthorGenre[i].text == \"Genre\"):\n",
    "                    authorGenreSearched = searchForAuthorGenre[i].find_next(\"div\", class_=\"dataItem\").text.split(',')[0]\n",
    "                    authorGenreSearched = authorGenreSearched.replace('\\n', '').strip()\n",
    "    \n",
    "        return (ISBNfound, [awardsSearched, authorGenreSearched, goodreadsLinksSeriesList])\n",
    "    \n",
    "    except:\n",
    "        return (False, [awardsSearched, authorGenreSearched, goodreadsLinksSeriesList])    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "xQu6DJm1Ayv2"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "collapsed": true,
    "executionInfo": {
     "elapsed": 807,
     "status": "ok",
     "timestamp": 1616238989096,
     "user": {
      "displayName": "Carine Therond",
      "photoUrl": "",
      "userId": "01656088634002615900"
     },
     "user_tz": -60
    },
    "id": "wETaFOq2Azq3"
   },
   "outputs": [],
   "source": [
    "def internetSearch(f, SaveFileName, theColumns, ind_start, ind_end):\n",
    "\n",
    "    NotFoundBooks = 0    \n",
    "    pdT = pd.DataFrame(columns = theColumns, dtype = object)\n",
    "    pdT['Same serie'] = pdT['Same serie'].astype(object)\n",
    "\n",
    "    for i in range(0, ind_start-1): \n",
    "        f.readline()\n",
    "        \n",
    "    csvLineParsed = ParserCsvInputBookCrossing(f.readline())\n",
    "    ret, refBook = GoogleSearchFunction(isbn = csvLineParsed[0])\n",
    "    pdT.loc[0] = refBook \n",
    "    pdT.loc[0:1].to_csv(SaveFileName, sep = ';', index = False, mode = 'w')\n",
    "    \n",
    "    #it doesn't start at 0 so that the trigger index 'len(pdT) - savingRate:len(pdT)+1' of save is correct\n",
    "    for i in range(ind_start+1, ind_end):    \n",
    "        retGoogle = False\n",
    "        retGoodReads = False\n",
    "        retPartial = False\n",
    "\n",
    "        lili = f.readline()\n",
    "        csvLineParsed = ParserCsvInputBookCrossing(lili)\n",
    "        #print(csvLineParsed)\n",
    "        \n",
    "        #First we use Google Books web site: search fastest\n",
    "        retGoogle, refBook = GoogleSearchFunction(isbn = csvLineParsed[0])\n",
    "        \n",
    "        if (retGoogle == False):\n",
    "            #if ISBN not found on Google Books, we try on Google Reads\n",
    "            retGoodReads, refBook = GoodReadsSearchFunction(isbn = csvLineParsed[0])\n",
    "            \n",
    "            if (retGoodReads == False):\n",
    "                #if ISBN not found (on Google Books and on Good Reads), \n",
    "                #we try search on GoodReads only a search with title and author \n",
    "                retGoodReads, refBook = GoodReadsSearchFunction(title = csvLineParsed[1], author = csvLineParsed[2].split(';')[-1])\n",
    "                if (retGoodReads == False):\n",
    "                    NotFoundBooks += 1\n",
    "                \n",
    "        else:\n",
    "           \n",
    "            #If ISBN found on Google Books, we complete some columns with GoodReads search\n",
    "            retPartial, refBookPartial = GoodReadsPartialSearchFunction(isbn = csvLineParsed[0])\n",
    "            if (retPartial == True):\n",
    "                refBook[-3:] = refBookPartial\n",
    "\n",
    "        pdT = pd.DataFrame(columns = theColumns, dtype = object)\n",
    "        pdT.loc[0] = refBook\n",
    "        pdT.loc[0:1].to_csv(SaveFileName, sep = ';', index = False, mode = 'a', header = False, encoding='utf-8')\n",
    "        #Google doesn't like when too much requests are performed...\n",
    "        #time.sleep(10.0)\n",
    "        \n",
    "    return NotFoundBooks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "dTqy-QoGA3D2"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": false,
    "executionInfo": {
     "elapsed": 72750,
     "status": "ok",
     "timestamp": 1616239074732,
     "user": {
      "displayName": "Carine Therond",
      "photoUrl": "",
      "userId": "01656088634002615900"
     },
     "user_tz": -60
    },
    "id": "bKJwNkgsA3Su",
    "outputId": "3d8d74e2-fbad-416b-988b-490dbb5b92f0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:20: UserWarning: Let this be your last warning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"0002005018\";\"Clara Callan\";\"Richard Bruce Wright\";\"2001\";\"HarperFlamingo Canada\";\"http://images.amazon.com/images/P/0002005018.01.THUMBZZZ.jpg\";\"http://images.amazon.com/images/P/0002005018.01.MZZZZZZZ.jpg\";\"http://images.amazon.com/images/P/0002005018.01.LZZZZZZZ.jpg\"\n",
      "\n",
      "\"0060973129\";\"Decision in Normandy\";\"Carlo D'Este\";\"1991\";\"HarperPerennial\";\"http://images.amazon.com/images/P/0060973129.01.THUMBZZZ.jpg\";\"http://images.amazon.com/images/P/0060973129.01.MZZZZZZZ.jpg\";\"http://images.amazon.com/images/P/0060973129.01.LZZZZZZZ.jpg\"\n",
      "\n",
      "\"0374157065\";\"Flu: The Story of the Great Influenza Pandemic of 1918 and the Search for the Virus That Caused It\";\"Gina Bari Kolata\";\"1999\";\"Farrar Straus Giroux\";\"http://images.amazon.com/images/P/0374157065.01.THUMBZZZ.jpg\";\"http://images.amazon.com/images/P/0374157065.01.MZZZZZZZ.jpg\";\"http://images.amazon.com/images/P/0374157065.01.LZZZZZZZ.jpg\"\n",
      "\n",
      "\"0393045218\";\"The Mummies of Urumchi\";\"E. J. W. Barber\";\"1999\";\"W. W. Norton &amp; Company\";\"http://images.amazon.com/images/P/0393045218.01.THUMBZZZ.jpg\";\"http://images.amazon.com/images/P/0393045218.01.MZZZZZZZ.jpg\";\"http://images.amazon.com/images/P/0393045218.01.LZZZZZZZ.jpg\"\n",
      "\n",
      "\"0399135782\";\"The Kitchen God's Wife\";\"Amy Tan\";\"1991\";\"Putnam Pub Group\";\"http://images.amazon.com/images/P/0399135782.01.THUMBZZZ.jpg\";\"http://images.amazon.com/images/P/0399135782.01.MZZZZZZZ.jpg\";\"http://images.amazon.com/images/P/0399135782.01.LZZZZZZZ.jpg\"\n",
      "\n",
      "\"0425176428\";\"What If?: The World's Foremost Military Historians Imagine What Might Have Been\";\"Robert Cowley\";\"2000\";\"Berkley Publishing Group\";\"http://images.amazon.com/images/P/0425176428.01.THUMBZZZ.jpg\";\"http://images.amazon.com/images/P/0425176428.01.MZZZZZZZ.jpg\";\"http://images.amazon.com/images/P/0425176428.01.LZZZZZZZ.jpg\"\n",
      "\n",
      "\"0671870432\";\"PLEADING GUILTY\";\"Scott Turow\";\"1993\";\"Audioworks\";\"http://images.amazon.com/images/P/0671870432.01.THUMBZZZ.jpg\";\"http://images.amazon.com/images/P/0671870432.01.MZZZZZZZ.jpg\";\"http://images.amazon.com/images/P/0671870432.01.LZZZZZZZ.jpg\"\n",
      "\n",
      "\"0679425608\";\"Under the Black Flag: The Romance and the Reality of Life Among the Pirates\";\"David Cordingly\";\"1996\";\"Random House\";\"http://images.amazon.com/images/P/0679425608.01.THUMBZZZ.jpg\";\"http://images.amazon.com/images/P/0679425608.01.MZZZZZZZ.jpg\";\"http://images.amazon.com/images/P/0679425608.01.LZZZZZZZ.jpg\"\n",
      "\n",
      "\"074322678X\";\"Where You'll Find Me: And Other Stories\";\"Ann Beattie\";\"2002\";\"Scribner\";\"http://images.amazon.com/images/P/074322678X.01.THUMBZZZ.jpg\";\"http://images.amazon.com/images/P/074322678X.01.MZZZZZZZ.jpg\";\"http://images.amazon.com/images/P/074322678X.01.LZZZZZZZ.jpg\"\n",
      "\n",
      "\"0771074670\";\"Nights Below Station Street\";\"David Adams Richards\";\"1988\";\"Emblem Editions\";\"http://images.amazon.com/images/P/0771074670.01.THUMBZZZ.jpg\";\"http://images.amazon.com/images/P/0771074670.01.MZZZZZZZ.jpg\";\"http://images.amazon.com/images/P/0771074670.01.LZZZZZZZ.jpg\"\n",
      "\n",
      "\"080652121X\";\"Hitler's Secret Bankers: The Myth of Swiss Neutrality During the Holocaust\";\"Adam Lebor\";\"2000\";\"Citadel Press\";\"http://images.amazon.com/images/P/080652121X.01.THUMBZZZ.jpg\";\"http://images.amazon.com/images/P/080652121X.01.MZZZZZZZ.jpg\";\"http://images.amazon.com/images/P/080652121X.01.LZZZZZZZ.jpg\"\n",
      "\n",
      "\"0887841740\";\"The Middle Stories\";\"Sheila Heti\";\"2004\";\"House of Anansi Press\";\"http://images.amazon.com/images/P/0887841740.01.THUMBZZZ.jpg\";\"http://images.amazon.com/images/P/0887841740.01.MZZZZZZZ.jpg\";\"http://images.amazon.com/images/P/0887841740.01.LZZZZZZZ.jpg\"\n",
      "\n",
      "\"1552041778\";\"Jane Doe\";\"R. J. Kaiser\";\"1999\";\"Mira Books\";\"http://images.amazon.com/images/P/1552041778.01.THUMBZZZ.jpg\";\"http://images.amazon.com/images/P/1552041778.01.MZZZZZZZ.jpg\";\"http://images.amazon.com/images/P/1552041778.01.LZZZZZZZ.jpg\"\n",
      "\n",
      "\"1558746218\";\"A Second Chicken Soup for the Woman's Soul (Chicken Soup for the Soul Series)\";\"Jack Canfield\";\"1998\";\"Health Communications\";\"http://images.amazon.com/images/P/1558746218.01.THUMBZZZ.jpg\";\"http://images.amazon.com/images/P/1558746218.01.MZZZZZZZ.jpg\";\"http://images.amazon.com/images/P/1558746218.01.LZZZZZZZ.jpg\"\n",
      "\n",
      "Not found books: 0\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "\n",
    "#cheminBookCrossing = '/content/drive/MyDrive/ProjetML/Base_BookCrossing/Base_originale/'\n",
    "cheminBookCrossing = '/home/ubuntu/BooksProject_GitHub/Books_project/datasets/'\n",
    "\n",
    "#cheminCommon = '/content/drive/MyDrive/ProjetML/Base_BookCrossing/Base_completee/'\n",
    "cheminCommon = '/home/ubuntu/BooksProject_GitHub/Books_project/datasets/'\n",
    "\n",
    "SaveFileName = cheminCommon + 'bothWebSites_InternetSearch_21_03_2021_30418___.csv'\n",
    "        \n",
    "theColumns = ['ISBN_10', 'ISBN_13', 'OtherID', 'Book-Title', 'Book-Author', \\\n",
    "                            'Year-Of-Publication', 'Publisher', 'Category', 'Description', 'Language', \\\n",
    "                            'Image', 'Pages', 'Awards', \"Author's genre\", 'Same serie']\n",
    "    \n",
    "#List of books to search on Google\n",
    "#f = open(cheminBookCrossing + 'BX-Books.csv', encoding=\"utf8\", errors=\"replace\")\n",
    "f = io.open(cheminBookCrossing + 'BX-Books.csv', encoding=\"utf8\", errors=\"replace\")\n",
    "f.readline()\n",
    "        \n",
    "#Internet search function\n",
    "with warnings.catch_warnings():\n",
    "    warnings.warn(\"Let this be your last warning\")\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    #NotFoundBooks = internetSearch(f, SaveFileName, theColumns, 30418, 270000) \n",
    "    NotFoundBooks = internetSearch(f, SaveFileName, theColumns, 0, 15) \n",
    "    \n",
    "print(\"Not found books: %d\" % NotFoundBooks)\n",
    "f.close()   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "id": "gHpYHNHLBqCd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/ubuntu/BooksProject_GitHub/Books_project/WebScrapper'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false,
    "id": "vusQpDaDAUdl"
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "EOL while scanning string literal (<ipython-input-137-f3ad4978c153>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-137-f3ad4978c153>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    \"\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m EOL while scanning string literal\n"
     ]
    }
   ],
   "source": [
    "\"\n",
    "                  1552041778\n",
    "                      (ISBN13: 9781552041772)\".split(\"ISBN13\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tot']"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test =\"tot\"\n",
    "test.split(\"ISBN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "http = urllib3.PoolManager()\n",
    "source = http.request('GET', https://www.goodreads.com/search?q=1552041778&qid=).data\n",
    "soup = bs4.BeautifulSoup(source, 'html.parser')\n",
    "ISBNfound = (soup.find(\"h1\", id=\"bookTitle\") != None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "http = urllib3.PoolManager()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "source = http.request('GET', \"https://www.goodreads.com/search?q=1552041778&qid=\").data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "soup = bs4.BeautifulSoup(source, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(soup.find(\"h1\", id=\"bookTitle\") != None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "searchForPublished = soup.find(\"div\", id=\"details\").find_all(\"div\", class_=\"row\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<div class=\"row\"><span itemprop=\"bookFormat\">Audio Cassette</span>, <span itemprop=\"bookEdition\">Abridged</span>, <span itemprop=\"numberOfPages\">0 pages</span></div>,\n",
       " <div class=\"row\">\\n            Published\\n        June 1st 1999\\n         by Mira Books\\n\\n      </div>]"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "searchForPublished"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "searchForPublished[1].find(\"nobr\", class_=\"greyText\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'        June 1st 1999         ', u' Mira Books']"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "searchForPublished[1].text.strip().replace('\\n', '').replace('Published', '').split('by')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "temp =searchForPublished[1].text.strip().replace('\\n', '').replace('Published', '').split('by')[0].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'June 1st 1999'"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'1999'"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.search(\"[0-9]{4,4}\", temp).group(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "                    if (searchForPublished[i].find(\"nobr\", class_=\"greyText\")):\n",
    "                        infoFirstPublished = searchForPublished[i].find(\"nobr\", class_=\"greyText\")\n",
    "                        tempPublished = searchForPublished[i].text.split(infoFirstPublished.text)\n",
    "                        tempPublished = tempPublished[0].strip().replace('\\n', '').replace('Published', '').split('by')\n",
    "                        pubData = tempPublished[0].strip()\n",
    "                        pubData = re.search(\"[0-9]{4,4}\", pubData)[0]\n",
    "                        pubSearched = tempPublished[1].strip()\n",
    "                    #Only current published date is provided\n",
    "                    else:\n",
    "                        tempPublished = searchForPublished[i].text.strip().replace('\\n', '').replace('Published', '').split('by')\n",
    "                        pubData = tempPublished[0].strip()\n",
    "                        pubData = re.search(\"[0-9]{4,4}\", pubData)[0]\n",
    "                        pubSearched = tempPublished[1].strip()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyN1/gwgxZpum9F4lQgRUXE5",
   "collapsed_sections": [],
   "name": "WebScrapper.ipynb",
   "provenance": [
    {
     "file_id": "1U7SQ5pkqZKGMIRzJNoW7kMBZm5UqhDA8",
     "timestamp": 1615796842051
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
