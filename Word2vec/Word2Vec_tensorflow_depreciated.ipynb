{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Word2Vec.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "350ee89b7c334755926e2b19e0b7bb29": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_99b1a7f5c70048219ac8686a82233c41",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_0ad6eb6fafbe49b6819b203ab9be7fcb",
              "IPY_MODEL_6a6cdf6b25664de99b55091c6b39c0ff"
            ]
          }
        },
        "99b1a7f5c70048219ac8686a82233c41": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "0ad6eb6fafbe49b6819b203ab9be7fcb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_10dce5a336cf4cffa000968bd982621b",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "danger",
            "max": 53476,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 53434,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_01834779b7854d3284527dd715aff478"
          }
        },
        "6a6cdf6b25664de99b55091c6b39c0ff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_b6d09aee9cab4d878ea0d43ef0050cf0",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 53434/53476 [20:01&lt;00:00, 51.43it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_42cda00e03444a4fab2b48ef86fc328b"
          }
        },
        "10dce5a336cf4cffa000968bd982621b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "01834779b7854d3284527dd715aff478": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b6d09aee9cab4d878ea0d43ef0050cf0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "42cda00e03444a4fab2b48ef86fc328b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "80e315396296402bae67df120e417366": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_c8836de6e643401ab6fa7771f22c0d7f",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_971cde5619434c4194eb13c904ff875c",
              "IPY_MODEL_5e4e44abb6794ac689659beb2ac191f6"
            ]
          }
        },
        "c8836de6e643401ab6fa7771f22c0d7f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "971cde5619434c4194eb13c904ff875c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_66319febf1d143e4b14806e74bdc242b",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 621620,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 621620,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_16c15b063ce9447e9f7e77b75808e269"
          }
        },
        "5e4e44abb6794ac689659beb2ac191f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_57237d92ca3947969dad9d60b13c6a8c",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 621620/621620 [11:41&lt;00:00, 886.10it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_7cb152cea26f48a68814ce15b0e2cb2a"
          }
        },
        "66319febf1d143e4b14806e74bdc242b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "16c15b063ce9447e9f7e77b75808e269": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "57237d92ca3947969dad9d60b13c6a8c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "7cb152cea26f48a68814ce15b0e2cb2a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2yDN-R0V7GFw"
      },
      "source": [
        "Based on [Tensorflow tutorial](https://www.tensorflow.org/tutorials/text/word2vec?hl=en#prepare_training_data_for_word2vec).\n",
        "\n",
        "To adapt w2v to book recomendation the books replace the words. List of rated books by an user is equivalent to a \"sentence\". The book \"numbering\" is already made by the database.\n",
        "\n",
        "Current issues :\n",
        "\n",
        "*   try to fit the book data in this implementation ==> only keep a fix amount of book for each other (10 books)\n",
        "  *  need to make more lists for each user in order to have all the book represented??\n",
        "Since every list is independent, need to make book appears several time for different list of each user with different books.\n",
        "  * Increase the size of each list to fit the longest user list and fill the smaller ones with 0?\n",
        "* book selection includes every rating of user. Need to remove books from bad ratings.\n",
        "*   Not sure what the *vocab_size* is supposed to be, the number of unique books in the data set is slightly too small.\n",
        "\n",
        "* Relevance of skip-gram? negative sampling bad here?\n",
        "\n",
        "* Only use one dataset, will switch to other \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D72-iBARfWf_"
      },
      "source": [
        "---\n",
        "# Books Implementation\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YXj3gjK5feaK"
      },
      "source": [
        "## W2V"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V3ZSQliF1XbM"
      },
      "source": [
        "import io\n",
        "import itertools # This module implements a number of iterator building blocks inspired by constructs from APL, Haskell (brrr), and SML\n",
        "import os\n",
        "import re #regex\n",
        "import string\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "\n",
        "import tqdm # make loops show a smart progress meter: 76%|████████████████████████       | 7568/10000 [00:33<00:10, 229.00it/s]\n",
        "import tqdm.notebook as tq # better for notebook\n",
        "\n",
        "\n",
        "# import timeit\n",
        "\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import Model, Sequential\n",
        "from tensorflow.keras.layers import Activation, Dense, Dot, Embedding, Flatten, GlobalAveragePooling1D, Reshape\n",
        "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
        "\n",
        "AUTOTUNE = tf.data.AUTOTUNE # need to read about tf Pipeline, and Prefetching\n",
        "SEED = 42 \n",
        "num_ns = 4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fZtzs1l2dTBD"
      },
      "source": [
        "**num_ns**: negative sampled context words that do not appear in the window size neighborhood of target_word\n",
        "\n",
        "A skip-gram model predicts the context (or neighbors) of a word, given the word itself. \n",
        "The training objective of the skip-gram model is to maximize the probability of predicting context words given the target word\n",
        "\n",
        "The Noise Contrastive Estimation loss function is an efficient approximation for a full softmax. \n",
        "With an objective to learn word embeddings instead of modelling the word distribution, \n",
        "NCE loss can be simplified to use negative sampling.\n",
        "\n",
        "The simplified negative sampling objective for a target word is to distinguish the context word from num_ns negative samples drawn from noise distribution of words. \n",
        "More precisely, an efficient approximation of full softmax over the vocabulary is, for a skip-gram pair, \n",
        "to pose the loss for a target word as a classification problem between the context word and num_ns negative samples.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "id": "c6JQvhHOxC_q",
        "outputId": "43935e50-5dec-46e9-c012-07f3ad4b8091"
      },
      "source": [
        "#import data\n",
        "#rated = pd.read_csv(\"https://raw.githubusercontent.com/ArielNATAF/Books_project/dev/datasets/ratings.csv\", usecols = [\"user_id\",\"book_id\",\"rating\"])\n",
        "rated = pd.read_csv(\"https://raw.githubusercontent.com/ArielNATAF/Books_project/dev/datasets/ratings.csv\", usecols = [\"user_id\",\"book_id\"])\n",
        "# the rated table has to be sorted for split_list()\n",
        "rated = rated.sort_values('user_id').reset_index(drop=True)\n",
        "\n",
        "books = pd.read_csv(\"https://raw.githubusercontent.com/ArielNATAF/Books_project/dev/datasets/books.csv\", usecols =['book_id', 'original_title'])\n",
        "rated.head()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>user_id</th>\n",
              "      <th>book_id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>258</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>140</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>869</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>2679</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>1310</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   user_id  book_id\n",
              "0        1      258\n",
              "1        1      140\n",
              "2        1      869\n",
              "3        1     2679\n",
              "4        1     1310"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yGMYo6FO5Bnb"
      },
      "source": [
        "# the rated table has to be sorted\n",
        "\n",
        "# for a fast implementation, we will ignore \n",
        "def split_users(n):\n",
        "\n",
        "  max_users = rated['user_id'].nunique()\n",
        "  max_id = rated['user_id'].nunique()\n",
        "  for id_user in tq.tqdm(range(1, max_users)):\n",
        "    count = 0\n",
        "    # j has to be the index of the first occurence of value of 'user' in rated['user_id']\n",
        "    # a bit dumb, since ordered j = rated.index[rated.user_id == id_user][0] = j+1\n",
        "    # but there might be exceptions\n",
        "\n",
        "    j = rated.index[rated.user_id == id_user][0]\n",
        "    if(rated['user_id'].iloc[j]):\n",
        "     new_id = rated['user_id'].iloc[j]\n",
        "\n",
        "    # table is sorted so check until no longer in the group\n",
        "    while rated['user_id'].iloc[j] == id_user:\n",
        "      # a bit redondant, \n",
        "      #if rated['user_id'].iloc[j] == id_user:\n",
        "        count = count + 1\n",
        "        rated['user_id'].iloc[j] = new_id\n",
        "        # increment value of the increment each n occurences\n",
        "        if count % n == 0:\n",
        "          # make sure new user value is unique\n",
        "          if new_id < max_id:\n",
        "            new_id = max_id\n",
        "          new_id = new_id + 1\n",
        "        j = j+1\n",
        "    max_id = new_id\n",
        "\n",
        "  return rated\n",
        "\n",
        "\n",
        "def pad_list(lst,n):\n",
        "  if not lst.size % n == 0:\n",
        "    lst = np.pad(lst, (0, 10 - (lst.size % 10)), mode='constant', constant_values=0)\n",
        "  return lst"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U2_ifLd0CCvA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e89e905-9cd1-4cd6-df42-30d4f048b2fe"
      },
      "source": [
        "a = rated.sort_values('user_id').reset_index(drop=True)\n",
        "print(a.head(35))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "    user_id  book_id\n",
            "0         1      258\n",
            "1         1       35\n",
            "2         1     4691\n",
            "3         1      238\n",
            "4         1     2063\n",
            "5         1      916\n",
            "6         1     4614\n",
            "7         1      111\n",
            "8         1       11\n",
            "9         1     1644\n",
            "10        1     3889\n",
            "11        1      136\n",
            "12        1     6665\n",
            "13        1       33\n",
            "14        1      138\n",
            "15        1       60\n",
            "16        1      148\n",
            "17        1       10\n",
            "18        1       94\n",
            "19        1      642\n",
            "20        1      492\n",
            "21        1       57\n",
            "22        1     1521\n",
            "23        1       70\n",
            "24        1       42\n",
            "25        1      103\n",
            "26        1      119\n",
            "27        1        4\n",
            "28        1       13\n",
            "29        1      212\n",
            "30        1       45\n",
            "31        1      233\n",
            "32        1      496\n",
            "33        1      306\n",
            "34        1      354\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402,
          "referenced_widgets": [
            "350ee89b7c334755926e2b19e0b7bb29",
            "99b1a7f5c70048219ac8686a82233c41",
            "0ad6eb6fafbe49b6819b203ab9be7fcb",
            "6a6cdf6b25664de99b55091c6b39c0ff",
            "10dce5a336cf4cffa000968bd982621b",
            "01834779b7854d3284527dd715aff478",
            "b6d09aee9cab4d878ea0d43ef0050cf0",
            "42cda00e03444a4fab2b48ef86fc328b"
          ]
        },
        "id": "3JYqjbM80mu_",
        "outputId": "d0bc2150-45cb-4aea-efdd-f62d369daa03"
      },
      "source": [
        "#Group the books of each user in lists in one column\n",
        "\n",
        "#rated_split = split_ratings(rated)\n",
        "#rated_split = rated_split.sort_values('user_id').reset_index(drop=True)\n",
        "n = 60\n",
        "rated = split_users(n)\n",
        "\n",
        "user_ratedbooks_ds = rated.groupby('user_id')['book_id'].apply(list).reset_index(name='book_list')\n",
        "user_ratedbooks_ds = user_ratedbooks_ds.drop('user_id', 1)\n",
        "user_ratedbooks_ds['book_list'].to_numpy()\n",
        "\n",
        "user_ratedbooks_ds['book_list'] = user_ratedbooks_ds['book_list'].apply(lambda x: np.array(x))\n",
        "user_ratedbooks_ds['book_list'] = user_ratedbooks_ds['book_list'].apply(lambda x: pad_list(x,n))\n",
        "# Pad list <n\n",
        "#user_ratedbooks_ds['book_list'] = user_ratedbooks_ds['book_list'].apply(lambda x: x[[0,1,2,3,4,5,6,7,8,9]])\n",
        "# ^^^ Bad way to only have n books for each list. Cut the \"excess\", lose data\n",
        "  # Just in case\n",
        "  \n",
        "# No user has less than n books rated (min=19) currently.\n",
        "  # function to check min/max sizes in ##Misc section\n",
        "\n",
        "print(user_ratedbooks_ds.head())\n",
        "print(type(user_ratedbooks_ds['book_list'][0]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "350ee89b7c334755926e2b19e0b7bb29",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=53476.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-128d86accd92>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#rated_split = rated_split.sort_values('user_id').reset_index(drop=True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m60\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mrated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplit_users\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0muser_ratedbooks_ds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrated\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'user_id'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'book_id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'book_list'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-8441e65b0264>\u001b[0m in \u001b[0;36msplit_users\u001b[0;34m(n)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;31m# but there might be exceptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrated\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrated\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_id\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mid_user\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'user_id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m      \u001b[0mnew_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'user_id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4102\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4103\u001b[0m             \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast_scalar_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwarn_float\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4104\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mgetitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4106\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: index 0 is out of bounds for axis 0 with size 0"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8bEiIGpOr1zO"
      },
      "source": [
        "# Generates skip-gram pairs with negative sampling for a list of sequences\n",
        "# (int-encoded sentences) based on window size, number of negative samples\n",
        "# and vocabulary size.\n",
        "def generate_training_data(sequences, window_size, num_ns, vocab_size, seed):\n",
        "  # Elements of each training example are appended to these lists.\n",
        "  targets, contexts, labels = [], [], []\n",
        "\n",
        "  # Build the sampling table for vocab_size tokens.\n",
        "  sampling_table = tf.keras.preprocessing.sequence.make_sampling_table(vocab_size)\n",
        "\n",
        "  # Iterate over all sequences (sentences) in dataset.\n",
        "  for sequence in tq.tqdm(sequences):\n",
        "\n",
        "    # Generate positive skip-gram pairs for a sequence (sentence).\n",
        "    positive_skip_grams, _ = tf.keras.preprocessing.sequence.skipgrams(\n",
        "          sequence, \n",
        "          vocabulary_size=vocab_size,\n",
        "          sampling_table=sampling_table,\n",
        "          window_size=window_size,\n",
        "          negative_samples=0)\n",
        "\n",
        "    # Iterate over each positive skip-gram pair to produce training examples \n",
        "    # with positive context word and negative samples.\n",
        "    for target_word, context_word in positive_skip_grams:\n",
        "      context_class = tf.expand_dims(\n",
        "          tf.constant([context_word], dtype=\"int64\"), 1)\n",
        "      negative_sampling_candidates, _, _ = tf.random.log_uniform_candidate_sampler(\n",
        "          true_classes=context_class,\n",
        "          num_true=1, \n",
        "          num_sampled=num_ns, \n",
        "          unique=True, \n",
        "          range_max=vocab_size, \n",
        "          seed=SEED, \n",
        "          name=\"negative_sampling\")\n",
        "\n",
        "      # Build context and label vectors (for one target word)\n",
        "      negative_sampling_candidates = tf.expand_dims(\n",
        "          negative_sampling_candidates, 1)\n",
        "\n",
        "      context = tf.concat([context_class, negative_sampling_candidates], 0)\n",
        "      label = tf.constant([1] + [0]*num_ns, dtype=\"int64\")\n",
        "\n",
        "      # Append each element from the training example to global lists.\n",
        "      targets.append(target_word)\n",
        "      contexts.append(context)\n",
        "      labels.append(label)\n",
        "\n",
        "  return targets, contexts, labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "004I5ngCJmHT",
        "outputId": "effc043f-6020-4390-c0c9-7584b5fe157a"
      },
      "source": [
        "# I am not sure what the vocab size is supposed to be,\n",
        "# the number of unique book is slightly too small here\n",
        "vocab_size = int(rated[\"book_id\"].nunique()*1.2) \n",
        "vocab      = books[\"original_title\"].tolist()\n",
        "type(books[\"original_title\"].tolist())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "list"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 82,
          "referenced_widgets": [
            "80e315396296402bae67df120e417366",
            "c8836de6e643401ab6fa7771f22c0d7f",
            "971cde5619434c4194eb13c904ff875c",
            "5e4e44abb6794ac689659beb2ac191f6",
            "66319febf1d143e4b14806e74bdc242b",
            "16c15b063ce9447e9f7e77b75808e269",
            "57237d92ca3947969dad9d60b13c6a8c",
            "7cb152cea26f48a68814ce15b0e2cb2a"
          ]
        },
        "id": "kt-3BXY1D4sZ",
        "outputId": "65beab10-ac0a-463f-fd34-53510b92dbf2"
      },
      "source": [
        "targets, contexts, labels = generate_training_data(\n",
        "    sequences   = user_ratedbooks_ds['book_list'], \n",
        "    window_size = 2, \n",
        "    num_ns      = 4, \n",
        "    vocab_size  = vocab_size, \n",
        "    seed        = SEED)\n",
        "print(len(targets), len(contexts), len(labels))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "80e315396296402bae67df120e417366",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=621620.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "6648479 6648479 6648479\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hHWaeIutD9ZQ"
      },
      "source": [
        "BATCH_SIZE = 1024\n",
        "BUFFER_SIZE = 10000\n",
        "dataset = tf.data.Dataset.from_tensor_slices(((targets, contexts), labels))\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
        "dataset = dataset.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "print(dataset)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wXtXOD-6I0zX"
      },
      "source": [
        "class Word2Vec(Model):\n",
        "  def __init__(self, vocab_size, embedding_dim):\n",
        "    super(Word2Vec, self).__init__()\n",
        "    self.target_embedding = Embedding(vocab_size, \n",
        "                                      embedding_dim,\n",
        "                                      input_length=1,\n",
        "                                      name=\"w2v_embedding\", )\n",
        "    self.context_embedding = Embedding(vocab_size, \n",
        "                                       embedding_dim, \n",
        "                                       input_length=num_ns+1)\n",
        "    self.dots = Dot(axes=(3,2))\n",
        "    self.flatten = Flatten()\n",
        "\n",
        "  def call(self, pair):\n",
        "    target, context = pair\n",
        "    we = self.target_embedding(target)\n",
        "    ce = self.context_embedding(context)\n",
        "    dots = self.dots([ce, we])\n",
        "    return self.flatten(dots)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FpgMoJjaI5jV",
        "outputId": "fb67f28f-562f-41d4-bde1-61824e435fb7"
      },
      "source": [
        "def custom_loss(x_logit, y_true):\n",
        "   return tf.nn.sigmoid_cross_entropy_with_logits(logits=x_logit, labels=y_true)\n",
        "\n",
        "\n",
        "embedding_dim = 128\n",
        "word2vec = Word2Vec(vocab_size, embedding_dim)\n",
        "word2vec.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=\"logs\")\n",
        "word2vec.fit(dataset, epochs=15, callbacks=[tensorboard_callback])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/15\n",
            "594/594 [==============================] - 26s 43ms/step - loss: 1.5334 - accuracy: 0.3026\n",
            "Epoch 2/15\n",
            "594/594 [==============================] - 26s 43ms/step - loss: 1.2640 - accuracy: 0.4388\n",
            "Epoch 3/15\n",
            "594/594 [==============================] - 25s 42ms/step - loss: 1.1478 - accuracy: 0.5335\n",
            "Epoch 4/15\n",
            "594/594 [==============================] - 22s 37ms/step - loss: 0.9952 - accuracy: 0.6364\n",
            "Epoch 5/15\n",
            "594/594 [==============================] - 25s 41ms/step - loss: 0.8668 - accuracy: 0.6978\n",
            "Epoch 6/15\n",
            "594/594 [==============================] - 22s 37ms/step - loss: 0.7533 - accuracy: 0.7498\n",
            "Epoch 7/15\n",
            "594/594 [==============================] - 24s 41ms/step - loss: 0.6474 - accuracy: 0.7981\n",
            "Epoch 8/15\n",
            "594/594 [==============================] - 22s 36ms/step - loss: 0.5504 - accuracy: 0.8421\n",
            "Epoch 9/15\n",
            "594/594 [==============================] - 24s 41ms/step - loss: 0.4647 - accuracy: 0.8789\n",
            "Epoch 10/15\n",
            "594/594 [==============================] - 24s 40ms/step - loss: 0.3911 - accuracy: 0.9083\n",
            "Epoch 11/15\n",
            "594/594 [==============================] - 25s 42ms/step - loss: 0.3290 - accuracy: 0.9309\n",
            "Epoch 12/15\n",
            "594/594 [==============================] - 23s 39ms/step - loss: 0.2769 - accuracy: 0.9474\n",
            "Epoch 13/15\n",
            "594/594 [==============================] - 26s 44ms/step - loss: 0.2335 - accuracy: 0.9600\n",
            "Epoch 14/15\n",
            "594/594 [==============================] - 26s 43ms/step - loss: 0.1972 - accuracy: 0.9692\n",
            "Epoch 15/15\n",
            "594/594 [==============================] - 24s 40ms/step - loss: 0.1669 - accuracy: 0.9762\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fa567a4a050>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RBCxEK1SMsZd"
      },
      "source": [
        "%load_ext tensorboard\n",
        "\n",
        "fashion_mnist = tf.keras.datasets.fashion_mnist\n",
        "\n",
        "(x_train, y_train),(x_test, y_test) = fashion_mnist.load_data()\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
        "\n",
        "%tensorboard --logdir logs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B43AmcxYNsAT"
      },
      "source": [
        "weights = word2vec.get_layer('w2v_embedding').get_weights()[0]\n",
        "vocab = books[\"original_title\"].tolist()\n",
        "\n",
        "out_v = io.open('vectors.tsv', 'w', encoding='utf-8')\n",
        "out_m = io.open('metadata.tsv', 'w', encoding='utf-8')\n",
        "\n",
        "for index, word in enumerate(vocab):\n",
        "  if  index == 0: continue # skip 0, it's padding.\n",
        "  vec = weights[index] \n",
        "  out_v.write('\\t'.join([str(x) for x in vec]) + \"\\n\")\n",
        "  out_m.write(str(word) + \"\\n\")\n",
        "out_v.close()\n",
        "out_m.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HcMSyd40bEmy"
      },
      "source": [
        "<\n",
        "\n",
        "< `vectors.tsv` & `metadata.tsv` in the notebook repo\n",
        "\n",
        "< \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ogpeIxq6ice",
        "outputId": "7b72cbbb-3e9c-470b-c790-297cd8268448"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "o5AKptp8N_Ap",
        "outputId": "77a1353a-bc61-45e0-bba0-f78e5cce3e5f"
      },
      "source": [
        "#download the files\n",
        "try:\n",
        "  from google.colab import files\n",
        "  files.download('vectors.tsv')\n",
        "  files.download('metadata.tsv')\n",
        "except Exception as e:\n",
        "  pass"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_4f7fbe6d-f4aa-425f-8614-2b97449e68ba\", \"vectors.tsv\", 14614420)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_ac66b28b-9cb2-46e3-84ae-1bcc08fc86cc\", \"metadata.tsv\", 232241)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oIEnkPdvfnbh"
      },
      "source": [
        "## Misc."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wFBiYgaWeIIo",
        "outputId": "23a5e29e-82d1-4d0e-eb85-a47b54ae0dbe"
      },
      "source": [
        "def check_min(ds=user_ratedbooks_ds['book_list']):\n",
        "  ds = ds.apply(lambda x: x.size)\n",
        "  print(\"Min size:\", ds.min())\n",
        "  print(\"Max size:\", ds.max())\n",
        "\n",
        "check_min()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Min size: 5976479\n",
            "Max size: 5976479\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TRrG9W76EuB_"
      },
      "source": [
        "def split_ratings(rated):\n",
        "  max_user = rated['user_id'].nunique()\n",
        "\n",
        "  rated_1 = rated.loc[rated['rating'] == 1].drop('rating', 1)\n",
        "  rated_2 = rated.loc[rated['rating'] == 2].drop('rating', 1)\n",
        "  rated_3 = rated.loc[rated['rating'] == 3].drop('rating', 1)\n",
        "  rated_4 = rated.loc[rated['rating'] == 4].drop('rating', 1)\n",
        "  rated_5 = rated.loc[rated['rating'] == 5].drop('rating', 1)\n",
        "\n",
        "  # add a function to be sure there is no missing index\n",
        "\n",
        "  frames = [rated_3, rated_4, rated_5]\n",
        "\n",
        "  split_rated = pd.concat(frames)\n",
        "  return split_rated\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xl2gPCD-hpkN"
      },
      "source": [
        "# MAYBE IT CAN BE COMPLETLY IGNORED IF BEFORE GROUPBY JUST CHANGE USER ID EVERY N BOOKS\n",
        "# (Still have to do the padding after groupby obviously)\n",
        "\n",
        "# Split lists of ds in list of 10:\n",
        "#   • Select a list in the ds (a list: a row in book_list column)\n",
        "#   • Check size lists of the ds\n",
        "#   • If not multiple of 10 itemss in list\n",
        "#       • Pad with 0 to multiple of 10 items\n",
        "#   • If more than 10 items in list\n",
        "#       • Copy last 10 items of list in new list\n",
        "#       • Remove last 10 items of current list\n",
        "#       • Add the new list to the ds\n",
        "#   • Go to next list\n",
        "\n",
        "# Need to be vastly optimized.\n",
        "#   • Create a  temp Series with number of cell to add,\n",
        "#   • Add the lists in each cells pf temp\n",
        "#   • append the filled temp after the loop\n",
        "# (should divide by 10 the number of appends)\n",
        "\n",
        "# Currently only a small part of dataset to save the RAM\n",
        "\n",
        "def split_list(ds):\n",
        "  print('count at start:',ds.count())\n",
        "  # count at start: 53424\n",
        "  start = timeit.default_timer()\n",
        "\n",
        "  print(ds[0].size,ds[1].size,ds[2].size,ds[3].size,ds[4].size)\n",
        "  ds = ds.apply(lambda x: pad_list(x))\n",
        "  print(ds[0].size,ds[1].size,ds[2].size,ds[3].size,ds[4].size)\n",
        "\n",
        "  for i in tq.tqdm(range(3000)):\n",
        "\t  while ds[i].size > 10:\n",
        "\t    size = ds[i].size\n",
        "\t    indice=[size-10,size-9,size-8,size-7,size-6,size-5,size-4,size-3,size-2,size-1]\n",
        "\t    new_list=ds[i][indice]\n",
        "\t    ds[i] = np.delete(ds[i],indice,0)\n",
        "\t    ds = ds.append(pd.Series([new_list]), ignore_index=True)\n",
        "\n",
        "  print('count at end:',ds.count())\n",
        "  # count at end: 621633\n",
        "    \n",
        "  stop = timeit.default_timer()\n",
        "  print('Time: ', int(stop - start)) # Time:  3200 => 53mn\n",
        "  return ds"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b4EiHHiu9SUD"
      },
      "source": [
        "---\n",
        "# TextExample\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JBu97C3HqDEB"
      },
      "source": [
        "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "970v2W3SqIT4"
      },
      "source": [
        "with open(path_to_file) as f: \n",
        "  lines = f.read().splitlines()\n",
        "for line in lines[:20]:\n",
        "  print(line)\n",
        "\n",
        "text_ds = tf.data.TextLineDataset(path_to_file).filter(lambda x: tf.cast(tf.strings.length(x), bool))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BarrBuDzqpFh"
      },
      "source": [
        "# We create a custom standardization function to lowercase the text and \n",
        "# remove punctuation.\n",
        "def custom_standardization(input_data):\n",
        "  lowercase = tf.strings.lower(input_data)\n",
        "  return tf.strings.regex_replace(lowercase,\n",
        "                                  '[%s]' % re.escape(string.punctuation), '')\n",
        "\n",
        "# Define the vocabulary size and number of words in a sequence.\n",
        "vocab_size = 4096\n",
        "sequence_length = 10\n",
        "\n",
        "# Use the text vectorization layer to normalize, split, and map strings to\n",
        "# integers. Set output_sequence_length length to pad all samples to same length.\n",
        "vectorize_layer = TextVectorization(\n",
        "    standardize=custom_standardization,\n",
        "    max_tokens=vocab_size,\n",
        "    output_mode='int',\n",
        "    output_sequence_length=sequence_length)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ofk6_Ec7qxXR"
      },
      "source": [
        "vectorize_layer.adapt(text_ds.batch(1024))\n",
        "type(vectorize_layer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W7vXlcPGq58j"
      },
      "source": [
        "# Save the created vocabulary for reference.\n",
        "inverse_vocab = vectorize_layer.get_vocabulary()\n",
        "print(inverse_vocab[:20])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8YK0vTwvrAfT"
      },
      "source": [
        "def vectorize_text(text):\n",
        "  text = tf.expand_dims(text, -1)\n",
        "  return tf.squeeze(vectorize_layer(text))\n",
        "\n",
        "# Vectorize the data in text_ds.\n",
        "text_vector_ds = text_ds.batch(1024).prefetch(AUTOTUNE).map(vectorize_layer).unbatch()\n",
        "text_vector_ds"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7aZQ9Z_IrcSL"
      },
      "source": [
        "sequences = list(text_vector_ds.as_numpy_iterator())\n",
        "print(len(sequences))\n",
        "for seq in sequences[:5]:\n",
        "  print(f\"{seq} => {[inverse_vocab[i] for i in seq]}\")\n",
        "  print(type (seq))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vZnga19TDnTl"
      },
      "source": [
        "# Generates skip-gram pairs with negative sampling for a list of sequences\n",
        "# (int-encoded sentences) based on window size, number of negative samples\n",
        "# and vocabulary size.\n",
        "def generate_training_data(sequences, window_size, num_ns, vocab_size, seed):\n",
        "  # Elements of each training example are appended to these lists.\n",
        "  targets, contexts, labels = [], [], []\n",
        "\n",
        "  # Build the sampling table for vocab_size tokens.\n",
        "  sampling_table = tf.keras.preprocessing.sequence.make_sampling_table(vocab_size)\n",
        "\n",
        "  # Iterate over all sequences (sentences) in dataset.\n",
        "  for sequence in tqdm.tqdm(sequences):\n",
        "\n",
        "    # Generate positive skip-gram pairs for a sequence (sentence).\n",
        "    positive_skip_grams, _ = tf.keras.preprocessing.sequence.skipgrams(\n",
        "          sequence, \n",
        "          vocabulary_size=vocab_size,\n",
        "          sampling_table=sampling_table,\n",
        "          window_size=window_size,\n",
        "          negative_samples=0)\n",
        "\n",
        "    # Iterate over each positive skip-gram pair to produce training examples \n",
        "    # with positive context word and negative samples.\n",
        "    for target_word, context_word in positive_skip_grams:\n",
        "      context_class = tf.expand_dims(\n",
        "          tf.constant([context_word], dtype=\"int64\"), 1)\n",
        "      negative_sampling_candidates, _, _ = tf.random.log_uniform_candidate_sampler(\n",
        "          true_classes=context_class,\n",
        "          num_true=1, \n",
        "          num_sampled=num_ns, \n",
        "          unique=True, \n",
        "          range_max=vocab_size, \n",
        "          seed=SEED, \n",
        "          name=\"negative_sampling\")\n",
        "\n",
        "      # Build context and label vectors (for one target word)\n",
        "      negative_sampling_candidates = tf.expand_dims(\n",
        "          negative_sampling_candidates, 1)\n",
        "\n",
        "      context = tf.concat([context_class, negative_sampling_candidates], 0)\n",
        "      label = tf.constant([1] + [0]*num_ns, dtype=\"int64\")\n",
        "\n",
        "      # Append each element from the training example to global lists.\n",
        "      targets.append(target_word)\n",
        "      contexts.append(context)\n",
        "      labels.append(label)\n",
        "\n",
        "  return targets, contexts, labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0I5Fcov9rsMO"
      },
      "source": [
        "targets, contexts, labels = generate_training_data(\n",
        "    sequences=sequences, \n",
        "    window_size=2, \n",
        "    num_ns=4, \n",
        "    vocab_size=vocab_size, \n",
        "    seed=SEED)\n",
        "print(len(targets), len(contexts), len(labels))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VdsbvB4FseoN"
      },
      "source": [
        "BATCH_SIZE = 1024\n",
        "BUFFER_SIZE = 10000\n",
        "dataset = tf.data.Dataset.from_tensor_slices(((targets, contexts), labels))\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
        "print(dataset)\n",
        "dataset = dataset.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "print(dataset)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jcSxFW1nshyU"
      },
      "source": [
        "class Word2Vec(Model):\n",
        "  def __init__(self, vocab_size, embedding_dim):\n",
        "    super(Word2Vec, self).__init__()\n",
        "    self.target_embedding = Embedding(vocab_size, \n",
        "                                      embedding_dim,\n",
        "                                      input_length=1,\n",
        "                                      name=\"w2v_embedding\", )\n",
        "    self.context_embedding = Embedding(vocab_size, \n",
        "                                       embedding_dim, \n",
        "                                       input_length=num_ns+1)\n",
        "    self.dots = Dot(axes=(3,2))\n",
        "    self.flatten = Flatten()\n",
        "\n",
        "  def call(self, pair):\n",
        "    target, context = pair\n",
        "    we = self.target_embedding(target)\n",
        "    ce = self.context_embedding(context)\n",
        "    dots = self.dots([ce, we])\n",
        "    return self.flatten(dots)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zGhnwp4hs2uN"
      },
      "source": [
        "def custom_loss(x_logit, y_true):\n",
        "      return tf.nn.sigmoid_cross_entropy_with_logits(logits=x_logit, labels=y_true)\n",
        "embedding_dim = 128\n",
        "\n",
        "word2vec = Word2Vec(vocab_size, embedding_dim)\n",
        "word2vec.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=\"logs\")\n",
        "word2vec.fit(dataset, epochs=20, callbacks=[tensorboard_callback])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ND5TDBoAtNoW"
      },
      "source": [
        "%load_ext tensorboard\n",
        "\n",
        "fashion_mnist = tf.keras.datasets.fashion_mnist\n",
        "\n",
        "(x_train, y_train),(x_test, y_test) = fashion_mnist.load_data()\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
        "\n",
        "%tensorboard --logdir logs\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
